{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. 데이터 불러오기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "raw_data = pd.read_csv('../data/data/filtered_data.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### contributor 4이상으로 필터링\n",
    "- 우리 네트워크의 엣지 기준은 co-contributor가 4 이상 \n",
    "- 이를 위해서는 반드시 한 repository에 최소 4명 이상의 contributor가 존재해야함 <br></br>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# contributor가 4명 이상인 repository만 고려 \n",
    "\n",
    "data = raw_data[raw_data.contributors_count>=4]\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "print('contributor가 4이상인 데이터 수 : {}'.format(len(data)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "contributor가 4이상인 데이터 수 : 3367\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Repository-contributor 사전 구축\n",
    "\n",
    "각 저장소 별, contributor를 담은 사전 자료구조를 생성 <br></br>\n",
    "\n",
    "- 의문점 : owner도 contributor에 포함시켜야할까?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# repo-contributor dictionary 생성\n",
    "repo_contributor_dict = {data.loc[row_idx, 'full_name'] : data.loc[row_idx, 'contributors'].split('#') for row_idx in range(len(data.contributors))}\n",
    "\n",
    "# repo에 owner 추가 \n",
    "for repo in list(repo_contributor_dict) :\n",
    "    repo_contributor_dict[repo].append(data[data.full_name==repo].owner.values[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Heterogeneous 네트워크 구축"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# repository 수, contributor 수 구하기 \n",
    "# 중복 제거된 유니크한 contributor 리스트 \n",
    "unique_contributor = []\n",
    "for contributors in repo_contributor_dict.values() :\n",
    "    unique_contributor += contributors\n",
    "unique_contributor = np.unique(unique_contributor)\n",
    "\n",
    "len_repo = len(data); len_contributor = len(unique_contributor)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# 네트워크 구축 \n",
    "hetero_network = pd.DataFrame(np.zeros((len_repo, len_contributor)), index=list(data.full_name), columns=unique_contributor)\n",
    "\n",
    "for k, v in tqdm(repo_contributor_dict.items()) :\n",
    "    for contributor in v :\n",
    "        hetero_network.loc[k, contributor] +=1 \n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 3367/3367 [00:09<00:00, 364.39it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. repository 네트워크 구축 "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "repository_network = hetero_network @ hetero_network.T"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Save network\n",
    "repository_network.to_csv('data/network/contributor_coupling.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def find_edge(network, repo) :\n",
    "    edge = []\n",
    "    \n",
    "    for i, element in enumerate(network.loc[repo, :]) :\n",
    "        if i == list(network).index(repo) :\n",
    "            diagonal = element\n",
    "        elif element != 0 :\n",
    "            edge.append((i, element))\n",
    "    \n",
    "    return edge, diagonal "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## repository 네트워크 의미 \n",
    "\n",
    "\n",
    "- diagonal term : 해당 노드가 가진 contributor의 수 \n",
    "- edge weight : 두 노드(저장소)의 공동 개발자의 수 "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- self loop를 제외하고는 weight 2인 링크 단 1개만 생성됨\n",
    "    - MachineLearningJournalClub/MLJC-UniTo-ProjectX-2020-public\n",
    "    - MachineLearningJournalClub/HowToTackleAMLCompetition\n",
    "    - edge weight 2\n",
    "- 실제 확인 결과 'Valerio Pagliarino', 'sazio' 두 개발자가 공동으로 개발에 참여\n",
    "    - 이 중 'sazio'는 'HowToTackleAMLCompetition'의 owner"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "# 엣지 기반 필터링 \n",
    "edge_threshold = 10\n",
    "repository_network = repository_network.where(repository_network>=edge_threshold, 0)\n",
    "\n",
    "# self loop 제거\n",
    "for i in range(len(repository_network)) :\n",
    "    repository_network.iloc[i, i] = 0\n",
    "\n",
    "# isolated 노드 확인\n",
    "nonisolated_node = []\n",
    "for node, edge in repository_network.iterrows() : \n",
    "     if sum(edge) != 0 :\n",
    "         nonisolated_node.append(node)\n",
    "\n",
    "# isolated 노드 제거 \n",
    "final_network = repository_network.loc[nonisolated_node, nonisolated_node]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "# 필터링된 네트워크 저장\n",
    "final_network.to_csv('data/contributor_' + str(edge_threshold) + '_filtered_network.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Coword 네트워크 구축 \n",
    "이를 기반으로 추후 co-word 및 word-coupling network 구축 "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "with open('../data/data/repo_topic_dict.pickle', 'rb') as f :\n",
    "    repo_topic_dict = pickle.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# 중복 의미를 가지는 키워드 통합\n",
    "'''''''''''\n",
    "machinelearning, machine-learning -> machine-learning\n",
    "deep-learning, deeplearning -> deep-learning\n",
    "'''''''''''\n",
    "\n",
    "change_word_dict = {'ml' : 'machine-learning', 'machine-learning-algorithms' : 'machine-learning', 'machinelearning' : 'machine-learning', 'machine-learning-models' : 'machine-learning', \n",
    "                    'machine' : 'machine-learning', 'learning' : 'machine-learning',\n",
    "                    'nlp' : 'natural-language-processing',\n",
    "                    'nlu' : 'natural-language-understanding',\n",
    "                    'deep-neural-networks' : 'deep-learning', 'deeplearning' : 'deep-learning', 'neural-network' : 'deep-learning', \n",
    "                    'edge' : 'edge-computing', 'edge-ai' : 'edge-computing', \n",
    "                    'python3' : 'python', \n",
    "                    'deep-reinforcement-learning' : 'reinforcement-learning', 'rl' : 'reinforcement-learning',\n",
    "                    'visualization' : 'data-visualization', \n",
    "                    'ai' : 'artificial-intelligence', \n",
    "                    'bot' : 'chatbot', \n",
    "                    'notebook' : 'jupyter-notebook', 'jupyter' : 'jupyter-notebook',\n",
    "                    'cnn' : 'convolutional-neural-networks',\n",
    "                    'automated-machine-learning' : 'automl', 'auto-ml' : 'automl',\n",
    "                    'explainable-ml' : 'explainable-ai', 'xai' : 'explainable-ai', \n",
    "                    'optimization' : 'hyperparameter-optimization', \n",
    "                    'datascience' : 'data-science', \n",
    "                    'big-data' : 'bigdata', \n",
    "                    'sklearn' : 'scikit-learn',\n",
    "                    'distributed' : 'distributed-computing', \n",
    "                    'sciml' : 'scientific-machine-learning',\n",
    "                    'differentialequations' : 'differential-equations', \n",
    "                    'segmentation' : 'image-segmentation', \n",
    "                    'gan' : 'generative-adversarial-network', \n",
    "                    'c-plus-plus' : 'cpp', \n",
    "                    'rnn' : 'recurrent-neural-networks',\n",
    "                    'tensorflow2' : 'tensorflow',\n",
    "                    'recommender-system' : 'recommendation-system'}\n",
    "\n",
    "stopwords = ['hacktoberfest2021', 'hacktoberfest', 'awesome', 'awesome-list']\n",
    "duplicated_words = list(change_word_dict.keys())\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# change words in 'repo_topic_dict'\n",
    "\n",
    "for k, v in repo_topic_dict.items() : \n",
    "    for i, word in enumerate(v) : \n",
    "        if word in duplicated_words : \n",
    "            repo_topic_dict[k][i] = change_word_dict[word]\n",
    "\n",
    "        if word in stopwords : \n",
    "            del repo_topic_dict[k][i]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# 토픽 유니크 리스트 만들기 \n",
    "topics = []\n",
    "\n",
    "for topic_corpus in repo_topic_dict.values() :\n",
    "    topics += topic_corpus \n",
    "\n",
    "unique_topics = list(set(topics))\n",
    "\n",
    "\n",
    "print('유니크한 토픽의 갯수 : {}'.format(len(unique_topics)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "유니크한 토픽의 갯수 : 6456\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# heterogeneous 네트워크 구축 \n",
    "repo_topic_network = pd.DataFrame(np.zeros((len(repo_topic_dict.keys()), len(unique_topics))), index=repo_topic_dict.keys(), columns=unique_topics)\n",
    "\n",
    "for k, v in tqdm(repo_topic_dict.items()) : \n",
    "    for topic_name in v :\n",
    "        repo_topic_network.loc[k, topic_name] += 1  "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 3367/3367 [00:01<00:00, 1841.86it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5-1. co-word 네트워크 구축 \n",
    "노드가 word인 네트워크 "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "coword = repo_topic_network.T @ repo_topic_network\n",
    "\n",
    "# diagonal term 제거 \n",
    "for topic in unique_topics : \n",
    "    coword.loc[topic, topic] = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "# 네트워크 저장하기 \n",
    "coword.to_csv('data/network/coword.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 경험적으로 edge weight를 8로 했을때 네트워크가 가장 깔끔하게 그려짐 "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# edge weight n이상인 네트워크만 남기기 \n",
    "edge_weight = 4\n",
    "filtered_coword = coword.where(coword>edge_weight, 0)\n",
    "\n",
    "# 네트워크 저장하기 \n",
    "filtered_coword.to_csv('data/network/' + str(edge_weight) +'_filtered_coword.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Network normalization using association strength"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](../ipynb_img/association_strength.png)\n",
    "\n",
    "Above image is equation of association strength. Here, $c_{ij}$ means value of each element. Also, $s_i$ denotes the i-th diagonal term of the co-occurrence matrix."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "network =repo_topic_network.T @ repo_topic_network"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Create a dictionary that stores the node name as a key and the value of the corresponding diagonal term as a value\n",
    "repo_diag_dict = {node : network.loc[node, node] for node in network.columns}\n",
    "\n",
    "# Association strength practice\n",
    "normalized_network = network.copy()\n",
    "\n",
    "for node1 in tqdm(network.columns) :\n",
    "    for node2 in network.columns :\n",
    "        normalized_network.loc[node1, node2] = network.loc[node1, node2] / (repo_diag_dict[node1] * repo_diag_dict[node2])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 6456/6456 [23:38<00:00,  4.55it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "normalized_network.to_csv('data/network/normalized_coword.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As a result of drawing the network by directly inputting it into gephi, it was confirmed that the most significant network was drawn when the threshold was 0.4."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "normalized_network = pd.read_csv('data/network/normalized_coword.csv', index_col=0)\n",
    "\n",
    "# edge weight n이상인 네트워크만 남기기 \n",
    "edge_weight = 0.5\n",
    "columns = normalized_network.columns\n",
    "normalized_network = normalized_network.values\n",
    "\n",
    "filtered_normalized_coword = np.where(normalized_network>edge_weight, 1, 0)\n",
    "filtered_normalized_coword = pd.DataFrame(filtered_normalized_coword, index=columns, columns=columns)\n",
    "\n",
    "# self-loop 제거\n",
    "for i in range(len(filtered_normalized_coword)) : \n",
    "    filtered_normalized_coword.iloc[i, i] = 0\n",
    "\n",
    "# 네트워크 저장하기 \n",
    "filtered_normalized_coword.to_csv('data/network/' + str(edge_weight) +'_filtered_normalized_coword.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5-2. word-coupling 네트워크 구축 "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "word_coupling = repo_topic_network @ repo_topic_network.T\n",
    "\n",
    "# diagonal term 제거 \n",
    "for repo in word_coupling.columns : \n",
    "    word_coupling.loc[repo, repo] = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# 네트워크 저장하기 \n",
    "word_coupling.to_csv('data/network/word_coupling.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 엣지의 수가 너무 많음;;; \n",
    "- 엣지 weight 8정도로 컷팅하고 이후 weight 조절하며 그림 확인하기 "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "# edge weight n이상인 네트워크만 남기기 \n",
    "edge_weight = 4\n",
    "filtered_word_coupling= word_coupling.where(word_coupling>edge_weight, 0)\n",
    "\n",
    "# 네트워크 저장하기 \n",
    "filtered_word_coupling.to_csv('data/network/' + str(edge_weight) +'_filtered_word_coupling.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Construct topic-repository network \n",
    "\n",
    "Topic quasi-networks have too many links to a particular topic, and are distinctly different from the original network.   \n",
    "\n",
    "This does not conform to the hypothesis that the predicted network will be a network at a future point in the original network.   \n",
    "\n",
    "Therefore, instead of applying the link prediction technique to the current quasi-network, we want to build the quasi-network again after applying the link prediction to the original network."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this code, we try to follow the network format proposed by TextGCN.   \n",
    "\n",
    "Nodes are composed of two types: topics and storage. \n",
    "\n",
    "An edge is created between the topic and the repository based on the existence of the topic.   \n",
    "\n",
    "The reason why tf-idf weights are not used here is that the number of appearances of topics per storage is fixed at 1.   \n",
    "\n",
    "That is, the only factor that affects the weight is idf, which gives a high weight to a topic that is not used often (such as a non-mainstream topic or a typo).    \n",
    "\n",
    "Therefore, in this network, binary edges are allocated based on the appearance of topics without using tf-idf weights.\n",
    "\n",
    "Also, there are edges between each topic. An edge is created between them based on mutual information.   \n",
    "\n",
    "The established network is as follows.\n",
    "\n",
    "![textgcn](../ipynb_img/textgcn_network.png)\n",
    "\n",
    "\n",
    "\n",
    "The approximate network structure is as follows.\n",
    "\n",
    "![network_form](../ipynb_img/textgcn.jpeg)\n",
    "\n",
    "In the above figure, zones 2 and 3 are repo-topic networks, so there is no need to implement them separately.\n",
    "\n",
    "ㅑ will call the top 1 and 2 zones as the upper network, and the bottom 3 and 4 zones as the lower network."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# get list data and return combination of each data \n",
    "# input : list\n",
    "# output : list of tuples \n",
    "def combination(data) :\n",
    "    output = []\n",
    "    for w1 in data :\n",
    "        for w2 in data :\n",
    "            if (w1 == w2) or ((w2, w1) in output) : \n",
    "                continue \n",
    "            output.append((w1, w2))\n",
    "\n",
    "    return output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# There are already constructred repo-topic network named 'repo_topic_network'\n",
    "# So, we should cacluate PPMI between topics \n",
    "# This is zone 1 of the above network.\n",
    "\n",
    "# Calculate probability of occurence per each topics \n",
    "n_repo, n_topic = repo_topic_network.shape\n",
    "\n",
    "total_words = []\n",
    "for v in repo_topic_dict.values() :\n",
    "    total_words += v \n",
    "\n",
    "occur_prob = {topic : total_words.count(topic)/n_repo for topic in list(repo_topic_network.columns)}\n",
    "\n",
    "# Calculate probabilty of co-occurence per topic pairs\n",
    "co_occur_prob = {}\n",
    "for corpus in repo_topic_dict.values() : \n",
    "    comb_of_words = combination(corpus)\n",
    "    for c in comb_of_words : \n",
    "        if c not in co_occur_prob.keys() :\n",
    "            co_occur_prob[c] = 1\n",
    "        else : \n",
    "            co_occur_prob[c] += 1 \n",
    "\n",
    "for k, v in co_occur_prob.items() : \n",
    "    co_occur_prob[k] = v/n_repo\n",
    "\n",
    "# Caluclate PPMI \n",
    "for k, v in co_occur_prob.items() :\n",
    "    co_occur_prob[k] = math.log10(v/(occur_prob[k[0]] * occur_prob[k[1]])) \n",
    "    if co_occur_prob[k] < 0 :\n",
    "        co_occur_prob[k] = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# Constuct topic-topic network \n",
    "ppmi_network = pd.DataFrame(np.zeros((n_topic, n_topic)), columns=repo_topic_network.columns, index=repo_topic_network.columns)\n",
    "\n",
    "for k, v in tqdm(co_occur_prob.items()) :\n",
    "    ppmi_network.loc[k[0], k[1]] = v\n",
    "    ppmi_network.loc[k[1], k[0]] = v"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 72860/72860 [00:04<00:00, 17091.73it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "# Create zone 4 in above network drawing\n",
    "repo_repo_network = pd.DataFrame(np.zeros((len(repo_topic_network.index), len(repo_topic_network.index))),\n",
    "                                    columns=repo_topic_network.index, index=repo_topic_network.index)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "# Construct upper network \n",
    "upper = pd.concat([ppmi_network, repo_topic_network.T], axis=1)\n",
    "\n",
    "# Construct lower network\n",
    "lower = pd.concat([repo_topic_network, repo_repo_network], axis=1)\n",
    "\n",
    "# Concat final textgcn network\n",
    "output_network = pd.concat([upper, lower]) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "# Save network \n",
    "repo_topic_network.to_csv('../data/network/repo_topic/repo_topic_network.csv')\n",
    "output_network.to_csv('../data/network/repo_topic/textgcn_network.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "repo_topic_network"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                             manifold-learning  odroid  driving-cars  \\\n",
       "PeterL1n/RobustVideoMatting                0.0     0.0           0.0   \n",
       "ml-tooling/opyrator                        0.0     0.0           0.0   \n",
       "4paradigm/OpenMLDB                         0.0     0.0           0.0   \n",
       "hora-search/hora                           0.0     0.0           0.0   \n",
       "salesforce/Merlion                         0.0     0.0           0.0   \n",
       "...                                        ...     ...           ...   \n",
       "BuggleInc/PLM                              0.0     0.0           0.0   \n",
       "rieck/sally                                0.0     0.0           0.0   \n",
       "jeremybarnes/jml                           0.0     0.0           0.0   \n",
       "shogun-toolbox/shogun-data                 0.0     0.0           0.0   \n",
       "matthiaskramm/mrscake                      0.0     0.0           0.0   \n",
       "\n",
       "                             alphago-zero  ordinary-differential-equations  \\\n",
       "PeterL1n/RobustVideoMatting           0.0                              0.0   \n",
       "ml-tooling/opyrator                   0.0                              0.0   \n",
       "4paradigm/OpenMLDB                    0.0                              0.0   \n",
       "hora-search/hora                      0.0                              0.0   \n",
       "salesforce/Merlion                    0.0                              0.0   \n",
       "...                                   ...                              ...   \n",
       "BuggleInc/PLM                         0.0                              0.0   \n",
       "rieck/sally                           0.0                              0.0   \n",
       "jeremybarnes/jml                      0.0                              0.0   \n",
       "shogun-toolbox/shogun-data            0.0                              0.0   \n",
       "matthiaskramm/mrscake                 0.0                              0.0   \n",
       "\n",
       "                             cognitive-search-engine  amazon  scala-library  \\\n",
       "PeterL1n/RobustVideoMatting                      0.0     0.0            0.0   \n",
       "ml-tooling/opyrator                              0.0     0.0            0.0   \n",
       "4paradigm/OpenMLDB                               0.0     0.0            0.0   \n",
       "hora-search/hora                                 0.0     0.0            0.0   \n",
       "salesforce/Merlion                               0.0     0.0            0.0   \n",
       "...                                              ...     ...            ...   \n",
       "BuggleInc/PLM                                    0.0     0.0            0.0   \n",
       "rieck/sally                                      0.0     0.0            0.0   \n",
       "jeremybarnes/jml                                 0.0     0.0            0.0   \n",
       "shogun-toolbox/shogun-data                       0.0     0.0            0.0   \n",
       "matthiaskramm/mrscake                            0.0     0.0            0.0   \n",
       "\n",
       "                             network-embedding  fast  ...  \\\n",
       "PeterL1n/RobustVideoMatting                0.0   0.0  ...   \n",
       "ml-tooling/opyrator                        0.0   0.0  ...   \n",
       "4paradigm/OpenMLDB                         0.0   0.0  ...   \n",
       "hora-search/hora                           0.0   0.0  ...   \n",
       "salesforce/Merlion                         0.0   0.0  ...   \n",
       "...                                        ...   ...  ...   \n",
       "BuggleInc/PLM                              0.0   0.0  ...   \n",
       "rieck/sally                                0.0   0.0  ...   \n",
       "jeremybarnes/jml                           0.0   0.0  ...   \n",
       "shogun-toolbox/shogun-data                 0.0   0.0  ...   \n",
       "matthiaskramm/mrscake                      0.0   0.0  ...   \n",
       "\n",
       "                             customer-journey-map  coco-dataset  \\\n",
       "PeterL1n/RobustVideoMatting                   0.0           0.0   \n",
       "ml-tooling/opyrator                           0.0           0.0   \n",
       "4paradigm/OpenMLDB                            0.0           0.0   \n",
       "hora-search/hora                              0.0           0.0   \n",
       "salesforce/Merlion                            0.0           0.0   \n",
       "...                                           ...           ...   \n",
       "BuggleInc/PLM                                 0.0           0.0   \n",
       "rieck/sally                                   0.0           0.0   \n",
       "jeremybarnes/jml                              0.0           0.0   \n",
       "shogun-toolbox/shogun-data                    0.0           0.0   \n",
       "matthiaskramm/mrscake                         0.0           0.0   \n",
       "\n",
       "                             watson-machine-learning  beam-search  \\\n",
       "PeterL1n/RobustVideoMatting                      0.0          0.0   \n",
       "ml-tooling/opyrator                              0.0          0.0   \n",
       "4paradigm/OpenMLDB                               0.0          0.0   \n",
       "hora-search/hora                                 0.0          0.0   \n",
       "salesforce/Merlion                               0.0          0.0   \n",
       "...                                              ...          ...   \n",
       "BuggleInc/PLM                                    0.0          0.0   \n",
       "rieck/sally                                      0.0          0.0   \n",
       "jeremybarnes/jml                                 0.0          0.0   \n",
       "shogun-toolbox/shogun-data                       0.0          0.0   \n",
       "matthiaskramm/mrscake                            0.0          0.0   \n",
       "\n",
       "                             medical-physics  gemm  early-warning-systems  \\\n",
       "PeterL1n/RobustVideoMatting              0.0   0.0                    0.0   \n",
       "ml-tooling/opyrator                      0.0   0.0                    0.0   \n",
       "4paradigm/OpenMLDB                       0.0   0.0                    0.0   \n",
       "hora-search/hora                         0.0   0.0                    0.0   \n",
       "salesforce/Merlion                       0.0   0.0                    0.0   \n",
       "...                                      ...   ...                    ...   \n",
       "BuggleInc/PLM                            0.0   0.0                    0.0   \n",
       "rieck/sally                              0.0   0.0                    0.0   \n",
       "jeremybarnes/jml                         0.0   0.0                    0.0   \n",
       "shogun-toolbox/shogun-data               0.0   0.0                    0.0   \n",
       "matthiaskramm/mrscake                    0.0   0.0                    0.0   \n",
       "\n",
       "                             textgraphs  swiftui  quality-assessment  \n",
       "PeterL1n/RobustVideoMatting         0.0      0.0                 0.0  \n",
       "ml-tooling/opyrator                 0.0      0.0                 0.0  \n",
       "4paradigm/OpenMLDB                  0.0      0.0                 0.0  \n",
       "hora-search/hora                    0.0      0.0                 0.0  \n",
       "salesforce/Merlion                  0.0      0.0                 0.0  \n",
       "...                                 ...      ...                 ...  \n",
       "BuggleInc/PLM                       0.0      0.0                 0.0  \n",
       "rieck/sally                         0.0      0.0                 0.0  \n",
       "jeremybarnes/jml                    0.0      0.0                 0.0  \n",
       "shogun-toolbox/shogun-data          0.0      0.0                 0.0  \n",
       "matthiaskramm/mrscake               0.0      0.0                 0.0  \n",
       "\n",
       "[3367 rows x 6456 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manifold-learning</th>\n",
       "      <th>odroid</th>\n",
       "      <th>driving-cars</th>\n",
       "      <th>alphago-zero</th>\n",
       "      <th>ordinary-differential-equations</th>\n",
       "      <th>cognitive-search-engine</th>\n",
       "      <th>amazon</th>\n",
       "      <th>scala-library</th>\n",
       "      <th>network-embedding</th>\n",
       "      <th>fast</th>\n",
       "      <th>...</th>\n",
       "      <th>customer-journey-map</th>\n",
       "      <th>coco-dataset</th>\n",
       "      <th>watson-machine-learning</th>\n",
       "      <th>beam-search</th>\n",
       "      <th>medical-physics</th>\n",
       "      <th>gemm</th>\n",
       "      <th>early-warning-systems</th>\n",
       "      <th>textgraphs</th>\n",
       "      <th>swiftui</th>\n",
       "      <th>quality-assessment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PeterL1n/RobustVideoMatting</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ml-tooling/opyrator</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4paradigm/OpenMLDB</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hora-search/hora</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salesforce/Merlion</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BuggleInc/PLM</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rieck/sally</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jeremybarnes/jml</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shogun-toolbox/shogun-data</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matthiaskramm/mrscake</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3367 rows × 6456 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9c1b964bfdaeba97df143313eacb83cb2a089d6aaed4d2c190fe89571ac71de"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('research': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}