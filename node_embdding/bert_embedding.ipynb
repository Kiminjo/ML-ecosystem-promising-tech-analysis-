{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Readme Embedding using BERT"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this code, the readme of the storage is embedded as a vector using **BERT**.\n",
    "\n",
    "There are two main reasons for using BERT.\n",
    "1. The readme does not preserve the order information.\n",
    "2. In general, BERT produces better sentence representation than doc2vec.\n",
    "\n",
    "In the readme used in this study, the order information of the text is not preserved due to the removal of the code, emoji and  the mixing of the title and the body.   \n",
    "\n",
    "However, doc2vec is a sequential language model that learns words based on their order. Therefore, the doc2vec model is not suitable for readme data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm \n",
    "import warnings \n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Load data \n",
    "data = pd.read_csv('data/data/filtered_data.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# The input data of BERT needs to be added with a special token.\n",
    "# Therefore, we add tokens before and after each sentence.\n",
    "corpus = list(data.readme)\n",
    "\n",
    "# text tokenize using BERT Tokenizer\n",
    "# split sentence into token \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenized_corpus = []\n",
    "for sent in tqdm(corpus) :\n",
    "    tokenized_corpus.append(tokenizer.tokenize(sent))\n",
    "\n",
    "# Map token to index \n",
    "token_index = [tokenizer.convert_tokens_to_ids(v) for v in tokenized_corpus]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 3367/3367 [00:54<00:00, 61.24it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# BERT trains by solving the next sentence prediction task along with MLM.\n",
    "# The same applies to prediction, so a sentence ID must be assigned to each sentence.\n",
    "# Since it is applied to a single sentence here, the ID of all sentences is unified as 1.\n",
    "segment_ids = []\n",
    "for token_corpus in token_index : \n",
    "    segment_ids.append([1] * len(token_corpus))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BERT practice \n",
    "\n",
    "I am new to BERT in this project. \n",
    "\n",
    "The code below is a summary of what I learned while implementing it, so you don't need to check below codes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# transform data to batch form \n",
    "batch_size = 16\n",
    "batch_len = int(len(corpus)/batch_size) + 1\n",
    "\n",
    "idx_list = []\n",
    "for i in range(10000) : \n",
    "    batch_final_idx = batch_size * i \n",
    "    idx_list.append(batch_final_idx)\n",
    "    if batch_final_idx >= len(corpus) :\n",
    "        break\n",
    "\n",
    "idx_list[-1] = -1\n",
    "\n",
    "print('number of batches : {}'.format(batch_len))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "number of batches : 211\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "total 27 batch are created.   \n",
    "\n",
    "Each batch has 128 corpus "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# The input data of BERT needs to be added with a special token.\n",
    "# Therefore, we add tokens before and after each sentence.\n",
    "corpus = list(data.readme)\n",
    "\n",
    "'''\n",
    "for i, sent in enumerate(corpus) : \n",
    "    corpus[i] = \"[CLS]\" + sent.lower() + \"[SEP]\"\n",
    "'''\n",
    "    \n",
    "# tokenize using 'batch_encode_plus' method \n",
    "# This method automates text embedding and sentence token assignment, which were manually implemented in the existing code implementation.\n",
    "bert_result = []\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "for i, idx in enumerate(idx_list) : \n",
    "    if i == batch_len - 1 :\n",
    "            break\n",
    "    \n",
    "    # visualize training process \n",
    "    print('{}/{}'.format(i, batch_len))\n",
    "\n",
    "    # tokenize corpus \n",
    "    tokenizer_test = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    token_output = tokenizer_test.batch_encode_plus(corpus[idx : idx_list[i+1]], padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "    # bert embedding using bert base model \n",
    "    result = bert_model(**token_output)\n",
    "    bert_result.append(result)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0/211\n",
      "1/211\n",
      "2/211\n",
      "3/211\n",
      "4/211\n",
      "5/211\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "101 is [CLS] token. \n",
    "\n",
    "Do not add '[CLS]' manually when using batch_encode_plus method. \n",
    "\n",
    "It automatically add special tokens."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Embedding using bert model \n",
    "# I don't know why add ** in parameter \n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "result = bert_model(**token_output)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Questions\n",
    "1. Why add ** in front of input sentence parameter in bert model instance?    \n",
    "      \n",
    "2. What is 'pooler-output' in bert model instance? and are there different output?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result.last_hidden_state.size()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result.last_hidden_state = result.last_hidden_state.resize_(4, 393216)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result.last_hidden_state"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('research': conda)"
  },
  "interpreter": {
   "hash": "b9c1b964bfdaeba97df143313eacb83cb2a089d6aaed4d2c190fe89571ac71de"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}